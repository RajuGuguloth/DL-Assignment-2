# -*- coding: utf-8 -*-
"""PartA.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NBMp1lUZTUO7kDIocRg2KKdvib4tp6a2
"""

import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import wandb
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt

# Initialize W&B
wandb.init(project='DL_Assignment')

# Define your CNN model
class CNNModel(nn.Module):
    def __init__(self, num_classes=10, filters=[32, 32, 32, 32, 32], activation="relu"):
        super(CNNModel, self).__init__()
        self.filters = filters
        self.activation = activation

        self.conv1 = nn.Conv2d(3, filters[0], kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(filters[0], filters[1], kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(filters[1], filters[2], kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(filters[2] * 8 * 8, 512)  # Example of a flattened layer
        self.fc2 = nn.Linear(512, num_classes)
        self.dropout = nn.Dropout(0.5)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        if self.activation == 'relu':
            activation_fn = torch.relu
        elif self.activation == 'elu':
            activation_fn = torch.elu
        elif self.activation == 'selu':
            activation_fn = torch.selu
        elif self.activation == 'silu':
            activation_fn = torch.silu
        elif self.activation == 'gelu':
            activation_fn = torch.gelu
        elif self.activation == 'mish':
            activation_fn = lambda x: x * torch.tanh(torch.nn.functional.softplus(x))  # Mish activation

        x = self.pool(activation_fn(self.conv1(x)))
        x = self.pool(activation_fn(self.conv2(x)))
        x = self.pool(activation_fn(self.conv3(x)))
        x = x.view(-1, self.filters[2] * 8 * 8)  # Flatten the tensor
        x = activation_fn(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return self.softmax(x)

# Data loading and preprocessing
def load_data(batch_size=32, augmentation="yes"):
    transform_list = [transforms.Resize((64, 64)), transforms.ToTensor(),
                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]

    if augmentation == "yes":
        transform_list.insert(0, transforms.RandomHorizontalFlip())
        transform_list.insert(0, transforms.RandomRotation(15))

    transform = transforms.Compose(transform_list)

    train_data = torchvision.datasets.ImageFolder(root='train/', transform=transform)
    test_data = torchvision.datasets.ImageFolder(root='test/', transform=transform)

    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader

# Train the model
def train_model(model, train_loader, criterion, optimizer, epochs=10):
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct, total = 0, 0

        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        avg_loss = running_loss / len(train_loader)
        accuracy = 100 * correct / total
        print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")

        # Log results to W&B
        wandb.log({"loss": avg_loss, "accuracy": accuracy})

# Test the model
def test_model(model, test_loader):
    model.eval()
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            all_labels.extend(labels.numpy())
            all_preds.extend(predicted.numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    print(f"Test Accuracy: {accuracy * 100:.2f}%")

    # Log results to W&B
    wandb.log({"test_accuracy": accuracy * 100})

    # Confusion Matrix
    cm = confusion_matrix(all_labels, all_preds)
    plot_confusion_matrix(cm)

# Plot Confusion Matrix
def plot_confusion_matrix(cm):
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.xticks(range(10))
    plt.yticks(range(10))
    plt.tight_layout()
    plt.show()

# Main function to orchestrate the workflow
def main(args):
    # Load data
    train_loader, test_loader = load_data(batch_size=args.batch_size, augmentation=args.augumentaion)

    # Initialize model, loss function, and optimizer
    model = CNNModel(num_classes=10, filters=args.num_filters, activation=args.activation)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)

    # Train the model
    train_model(model, train_loader, criterion, optimizer, epochs=args.epochs)

    # Test the model
    test_model(model, test_loader)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--wandb_entity", "-we",help="Wandb Entity used to track experiments in the Weights & Biases dashboard.", default="cs23m065")
    parser.add_argument("--wandb_project", "-wp",help="Project name used to track experiments in Weights & Biases dashboard", default="Assignment 2")
    parser.add_argument("--epochs","-e", help="Number of epochs to train neural network", type=int, default=10)
    parser.add_argument("--batch_size","-b", help="Batch size used to train neural network", type=int, default=16)
    parser.add_argument("--optimizer","-o", help="Batch size is used to train neural network", default="nadam", choices=['nadam', 'adam', 'rmsprop'])
    parser.add_argument("--learning_rate","-lr", default=0.1, type=float)
    parser.add_argument("--weight_decay","-w_d", default=0.0, type=float)
    parser.add_argument("--activation", "-a", choices=['relu', 'elu', 'selu', 'silu', 'gelu', 'mish'], default="relu")
    parser.add_argument("--num_filters", "-nf", nargs=5, type=int, default=[32, 32, 32, 32, 32])
    parser.add_argument("--filter_sizes", "-fs", nargs=5, type=int, default=[3, 3, 3, 3, 3])
    parser.add_argument("--batch_norm", "-bn", default="true", choices=["true", "false"])
    parser.add_argument("--dense_layer", "-dl", default=128, type=int)
    parser.add_argument("--augumentaion", "-a", default="yes", choices=["yes", "no"])
    parser.add_argument("--dropout", "-dp", default=0.2, type=float)
    parser.add_argument("--base_dir", "-br", default="inaturalist_12K")

    args = parser.parse_args()

    # Run the main function with the arguments
    main(args)