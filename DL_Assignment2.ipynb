{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpy12C8sV19U+ofKd3GVVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajuGuguloth/DL-Assignment-2/blob/main/DL_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup and Imports**"
      ],
      "metadata": {
        "id": "y0SOrSNRJaBl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2v0SNew-xa8"
      },
      "outputs": [],
      "source": [
        " # üì¶ Required installations\n",
        "!pip install -q wandb\n",
        "\n",
        "# üìö Importing essential libraries\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **W&B Login and Device Setup**"
      ],
      "metadata": {
        "id": "T-kbL_ZsJW8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üßë‚Äçüíª W&B Login\n",
        "wandb.login()\n",
        "\n",
        "# üñ•Ô∏è Device Setup (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "xY1E1QKdLm11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "dqtJkzqKLttA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÇ Dataset Path Setup\n",
        "data_dir = 'path_to_your_data_directory'\n",
        "\n",
        "# üîÑ Image Transformations for Data Augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resizing the image to a standard size\n",
        "    transforms.ToTensor(),  # Converting images to tensor format\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizing based on ImageNet statistics\n",
        "])\n",
        "\n",
        "# üìÇ Loading Data with ImageFolder\n",
        "dataset = ImageFolder(root=data_dir, transform=transform)\n",
        "\n",
        "# üîÄ Splitting Dataset into Training and Validation Sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# üßë‚Äçüè´ DataLoader for Batch Loading\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)} | Validation set size: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "id": "9oC4pfIgL0wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Model Definition**"
      ],
      "metadata": {
        "id": "mUlaUDeyL3ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üèóÔ∏è Defining the CNN Architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Convolutional Layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(128*16*16, 512)  # Flattening the image after convolutions\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "        # Dropout Layer for regularization\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        # Flatten the output from convolutional layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))  # Fully connected layer\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc2(x)  # Final output layer\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zMILYlr_L9S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Function and Optimizer Setup**"
      ],
      "metadata": {
        "id": "K-oHmbZ5MAVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öñÔ∏è Loss Function: Cross-Entropy Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# üõ†Ô∏è Optimizer: Adam with weight decay (L2 regularization)\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n"
      ],
      "metadata": {
        "id": "CId0f3dnMJC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}