{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d112236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üì¶ Importing Required Libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73379097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÅ Dataset Preparation\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/wandb_datasets/nature_12K.zip\"\n",
    "dataset_zip = \"nature_12K.zip\"\n",
    "extract_path = \"./nature_dataset\"\n",
    "\n",
    "# Download and extract dataset if not already done\n",
    "if not os.path.exists(extract_path):\n",
    "    urllib.request.urlretrieve(dataset_url, dataset_zip)\n",
    "    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    os.remove(dataset_zip)\n",
    "    print(\"Dataset ready at:\", extract_path)\n",
    "else:\n",
    "    print(\"Dataset already available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198cc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üß™ Custom Data Transformations\n",
    "def image_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîÑ Load and Split Data\n",
    "def get_dataloaders(data_dir, batch_size=32, val_ratio=0.2):\n",
    "    dataset = datasets.ImageFolder(data_dir, transform=image_transforms())\n",
    "    total = len(dataset)\n",
    "    val_size = int(total * val_ratio)\n",
    "    train_size = total - val_size\n",
    "\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, dataset.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db361277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üß† Create ResNet Model\n",
    "def build_model(num_classes):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406db79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üèãÔ∏è Training Procedure\n",
    "def train_model(model, dataloader, optimizer, criterion, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        acc = correct / total * 100\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}, Accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìä Evaluation Function\n",
    "def evaluate_model(model, dataloader, class_names):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in dataloader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(lbls.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, xticklabels=class_names, yticklabels=class_names, fmt='d', cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e49659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üöÄ Run Training Pipeline\n",
    "DATA_DIR = \"./nature_dataset/inaturalist_12K/train\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "train_loader, val_loader, labels = get_dataloaders(DATA_DIR, BATCH_SIZE)\n",
    "cnn = build_model(len(labels))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.fc.parameters(), lr=0.001)\n",
    "\n",
    "train_model(cnn, train_loader, optimizer, loss_fn, EPOCHS)\n",
    "evaluate_model(cnn, val_loader, labels)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
